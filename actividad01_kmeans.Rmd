---
title: "Análisis de la estabilidad de los centroides en K-Medias en presencia de correlación"
author: "Juan David Ospina Arango <br/> Universidad Nacional de Colombia - Sede Medellín <br/> Departamento de Ciencias de la Computación y de la Decisión <br/> Decisiones bajo incertidumbre (Optimización para aprendizaje de máquina)"
date: "Semestre 2021-01"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, cache = TRUE)
library(mvtnorm)
library(MBESS)
library(Matrix)
library(factoextra)
```

### El algoritmo de K-Means

El algoritmo de [K-Means](https://en.wikipedia.org/wiki/K-means_clustering) es una técnica popular de aprendizaje no supervisado para agrupar observaciones. 

Uno de los retos en la aplicación de los métodos de aprendizaje de máquinas es el manejo de información redundante. Se considera que la información es redundante cuando a partir de unas variables se pueden inferir las otras.

Un ejemplo de redundancia es la correlación alta entre variables. Si dos variables están altamente correlacionadas, conocer lo que pasa con una permite saber lo que pasa con la otra. Este problema también se conoce como colinealidad. 

Por otro lado, la estabilidad de un método de aprendizaje de máquina se puede entender de diferentes maneras. Cambios pequeños en el conjunto de entrenamiento no producen cambios significativos en:
* a) en los parámetros estimados del modelo (estabilidad en los parámetros) o 
* b) en las salidas del modelo (cambio en las predicciones del modelo)

Uno de los retos de la redundancia es que puede afectar la estabilidad de los métodos de aprendizaje de máquina. En particular, en K-Medias la estabilidad se puede establecer como la variabilidad de los centroides finales cada vez que se cambian los centroides iniciales. Cuando cambiar los centroides iniciales no modifica los centroides finales, se puede considerar que el método tiene un comportamiento estable respecto a la inicialización.


### Objetivo
Entender cómo la correlación entre las variables numéricas puede afectar la estabilidad de los centroides en el algoritmo de K-Medias utilizando escenarios de simulación.

### Retos de aprendizaje
* Planteamiento de estudios de simulación
* Refuerzo de los conceptos estadísticos de media, varianza, covarianza y correlación, distribución normal multivariada
* Refuerzo del algoritmo de K-Medias


### Metodología
Se deberá desarrollar un experimento de simulación para analizar la estabilidad del algoritmo de K-Means. Para ello se proponen los siguientes pasos.

1. Simular tres grupos de distribuciones normales bivariadas independientes pero con traslape. Es decir que los miembros de cada grupo son $X\sim N_2(\mu_j,\Sigma_j)$, $j=1,2,3$. A continuación se presenta un ejemplo de dos grupos generados a partir de distribuciones normales bivariadas:

```{r}
M_cor<-matrix(c(1,0.6,0.6,1),ncol=2)

M_cov<-cor2cov(M_cor,sd=c(1,1))

M_cov_pd<-as.matrix(nearPD(M_cov)$mat)
n1<-50 # Tamaño de la muestra de la clase 1
n2<-80 # Tamaño de la muestra de la clase 2
n3<-100 # Tamaño de la muestra de la clase 3
mu3<-c(0.5,1.5) # Vector de medias de la clase 1
mu4<-c(-1,1.5) # Vector de medias de la clase 2
mu5<-c(-0.8,1) # Vector de medias de la clase 3
set.seed(1)
muestra3<-rmvnorm(n=n1,mean=mu3,sigma=M_cov_pd,method="eigen")
muestra4<-rmvnorm(n=n2,mean=mu4,sigma=M_cov_pd,method="eigen")
muestra5<-rmvnorm(n=n3,mean=mu5,sigma=M_cov_pd,method="eigen")
muestra_nosep<-rbind(muestra3,muestra4,muestra5)
clase<-c(rep(-1,n1),rep(1,n2),rep(3,n3))
muestra_nosep_df<-data.frame(muestra_nosep,clase)

plot(muestra_nosep,
     col=(clase+2),
     pch=(clase+2),
     ylim=c(-2,6),xlim=c(-6,6),
     xaxt="n",yaxt="n",
     xlab=expression(x[1]),
     ylab=expression(x[2]),
     main="Ejemplo de dos grupos que se traslapan",
     las=1,cex=1.5,lwd=2)
grid()
legend("topleft",legend=c("Grupo 1", "Grupo 2", "Grupo 3"),
       pch=c(1,3,5),col=c(1,3,5),
       pt.lwd=2,pt.cex=1.8,bty="n")
```

2. Encontrar los centroides con K-Means fijando el método de inicialización de los centroides. Encuentre los centroides para $n_c$ inicializaciones aleatorias.
```{r}
n_c=sample(1:100,replace=F)
#set.seed(1)
k1 <- kmeans(muestra_nosep_df, 3, nstart = sample(1:100,size = 1))
k2 <- kmeans(muestra_nosep_df, 3, nstart = sample(1:100,size = 1))
k3 <- kmeans(muestra_nosep_df, 3, nstart = sample(1:100,size = 1))
k4 <- kmeans(muestra_nosep_df, 3, nstart = sample(1:100,size = 1))
#print(k1$centers)
cent <- list(k1 = k1$centers,k2 = k2$centers,k3 = k3$centers,k4 = k4$centers)
print(cent)
p1 <- fviz_cluster(k1, geom = "point", data = muestra_nosep_df) + ggtitle("k = 3")
p2 <- fviz_cluster(k2, geom = "point", data = muestra_nosep_df) + ggtitle("k = 3")
p3 <- fviz_cluster(k3, geom = "point", data = muestra_nosep_df) + ggtitle("k = 3")
p4 <- fviz_cluster(k4, geom = "point", data = muestra_nosep_df) + ggtitle("k = 3")
library(gridExtra)
grid.arrange(p1, p2, p3, p4, nrow = 2)
#fviz_cluster(km.res, data = muestra_nosep_df)
```

3. Suponga que el $i$-ésimo individuo es de la forma $X_i = [x_1^i \quad x_2^i]^T$. Cree la variable $x_3$ como $x_3^i=x_1^i+\epsilon_i$ con $\epsilon_i$ iid de media cero y varianza constante. ¿Cuál es la varianza de $x_3$? ¿Cuál es la covarianza entre $x_1$ y $x_3$? ¿Al agregar esta variable K-Means sigue detectando correctamente los centroides? ¿Qué pasa con la estabilidad de los centroides cuando la varianza de $\epsilon$ aumenta?
```{r}
e<-rmvnorm(n=n1,mean=c(0,0),sigma=M_cov_pd,method="eigen")
x3 <- muestra3 + e
var(x3, y = NULL, na.rm = FALSE)
cov(muestra3, y = x3, use = "everything",
    method = c("pearson", "kendall", "spearman"))
muestra_p3<-rbind(muestra3,muestra4,muestra5,x3)
clasep3<-c(rep(-1,n1),rep(1,n2),rep(3,n3),rep(5,n1))
muestra_p3_df<-data.frame(muestra_p3,clasep3)
p3k1 <- kmeans(muestra_p3_df, 4, nstart = sample(1:100,size = 1))
p3k2 <- kmeans(muestra_p3_df, 4, nstart = sample(1:100,size = 1))
p3k3 <- kmeans(muestra_p3_df, 4, nstart = sample(1:100,size = 1))
p3k4 <- kmeans(muestra_p3_df, 4, nstart = sample(1:100,size = 1))
#print(k1$centers)
cent <- list(k1 = p3k1$centers,k2 = p3k2$centers,k3 = p3k3$centers,k4 = p3k4$centers)
print(cent)
p13 <- fviz_cluster(p3k1, geom = "point", data = as.data.frame(muestra_p3)) + ggtitle("k = 4")
p23 <- fviz_cluster(p3k2, geom = "point", data = as.data.frame(muestra_p3)) + ggtitle("k = 4")
p33 <- fviz_cluster(p3k3, geom = "point", data = as.data.frame(muestra_p3)) + ggtitle("k = 4")
p43 <- fviz_cluster(p3k4, geom = "point", data = as.data.frame(muestra_p3)) + ggtitle("k = 4")
library(gridExtra)
grid.arrange(p13, p23, p33, p43, nrow = 2)

```

4. Como en el paso anterior, cree las variables $x_4$ y $x_6$ como la suma de $x_2$ y otra variable de media cero y varianza constante y la variable $x_5$ como la suma de $x_3$ y otra variable de media cero y varianza constante. ¿Al agregar estas variables, K-Means sigue detectando correctamente los centroides? ¿Qué pasa cuando la estabilidad de los centroides cuando la varianza de las variables que se suman a las variables originales aumenta?
```{r}
e4<-rmvnorm(n=80,mean=c(0,0),sigma=M_cov_pd,method="eigen")
x4 <- muestra4 + e4
e6<-rmvnorm(n=80,mean=c(0,0),sigma=M_cov_pd,method="eigen")
x6 <- muestra4 + e6
e5<-rmvnorm(n=50,mean=c(0,0),sigma=M_cov_pd,method="eigen")
x5 <- x3 + e5

muestra_pt<-rbind(muestra3,muestra4,muestra5,x3,x4,x5,x6)
clasept<-c(rep(-1,n1),rep(1,n2),rep(3,n3),rep(5,50),rep(7,80),rep(9,80),rep(11,50))
muestra_pt_df<-data.frame(muestra_pt,clasept)
ptk1 <- kmeans(muestra_pt_df, 7, nstart = sample(1:100,size = 1))
ptk2 <- kmeans(muestra_pt_df, 7, nstart = sample(1:100,size = 1))
ptk3 <- kmeans(muestra_pt_df, 7, nstart = sample(1:100,size = 1))
ptk4 <- kmeans(muestra_pt_df, 7, nstart = sample(1:100,size = 1))
#print(k1$centers)
cent <- list(k1 = ptk1$centers,k2 = ptk2$centers,k3 = ptk3$centers,k4 = ptk4$centers)
print(cent)
p1t <- fviz_cluster(ptk1, geom = "point", data = as.data.frame(muestra_pt)) + ggtitle("k = 7")
p2t <- fviz_cluster(ptk2, geom = "point", data = as.data.frame(muestra_pt)) + ggtitle("k = 7")
p3t <- fviz_cluster(ptk3, geom = "point", data = as.data.frame(muestra_pt)) + ggtitle("k = 7")
p4t <- fviz_cluster(ptk4, geom = "point", data = as.data.frame(muestra_pt)) + ggtitle("k = 7")
library(gridExtra)
grid.arrange(p1t, p2t, p3t, p4t, nrow = 2)
```

5. A partir de estos experimentos, ¿qué se podría decir del efecto de la correlación entre variables y la estabilidad de los centroides en K-Medias?

## Por la existencia de la correlación y covarianza entre las variables de las distribuciones, el k-means no logra diferenciar correctamente los grupos. Además, con el ingreso de las últimas variables, se evidencian valores lejanos que halan los centrodies de los clusters en dicha dirección, afectando su diferenciación.


### Equipos y fecha de entrega
1. El trabajo se realizará en equipos de tres a cinco estudiantes.
2. El código generado para el estudio se debe publicar en GitHub.
3. El reporte se debe publicar como una entrada de blog (por ejemplo en GitHub o [Rpubs](https://rpubs.com)). Debe contener:
 + Planteamiento del problema
 + Descripción de la metodología
 + Resultados
 + Bibliografía
4. La entrega se hará a través de la plataforma Google Classroom del curso en el espacio correspondiente para ello.


